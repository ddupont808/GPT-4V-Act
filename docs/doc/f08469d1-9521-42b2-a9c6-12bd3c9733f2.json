{
    "summary": "GPT-4V-Act is a tool that combines GPT-4V and Set-of-Mark Prompting for UI automation, offering accessibility improvements, workflow automation, and testing. It requires ChatGPT Plus, has an auto-labeler for UI elements, defines action types and response formats for website tasks, and provides demonstration prompts with JSON markdown output.",
    "details": [
        {
            "comment": "This is a project that utilizes GPT-4V(ision) and a web browser to create a smooth human-computer interface for various tasks, such as UI accessibility improvements, workflow automation, and automated UI testing. Requires an active ChatGPT Plus subscription due to unofficial API usage.",
            "location": "\"/media/root/Toshiba XG3/works/GPT-4V-Act/docs/src/README.md\":0-5",
            "content": "# GPT-4V-Act: Chromium Copilot\n> \u26a0\ufe0f **Important Note:** As GPT-4V(ision) has not yet been made publicly available, this project necessitates an active ChatGPT Plus subscription for multimodal prompting access. It's worth noting that the tactics used by this project to tap into an unofficial GPT-4V API may contravene the associated ChatGPT Term of Service clause:\n>> **2.** (c) **Restrictions:**  You may not ... (iv) except as permitted through the API, use any automated or programmatic method to extract data or output from the Services, including scraping, web harvesting, or web data extraction;\nGPT-4V-Act serves as an eloquent multimodal AI assistant that harmoniously combines GPT-4V(ision) with a web browser. It's designed to mirror the input and output of a human operator\u2014primarily screen feedback and low-level mouse/keyboard interaction. The objective is to foster a smooth transition between human-computer operations, facilitating the creation of tools that considerably boost the accessibility of any user interface (UI), aid workflow automation, and enable automated UI testing."
        },
        {
            "comment": "GPT-4V-Act is a tool that uses GPT-4V and Set-of-Mark Prompting to automate UI interactions. It has an auto-labeler for assigning numerical IDs to UI elements, and can perform actions based on tasks and screenshots. Clone the repo, navigate to it, install packages, and start the demo.",
            "location": "\"/media/root/Toshiba XG3/works/GPT-4V-Act/docs/src/README.md\":8-38",
            "content": "https://github.com/ddupont808/GPT-4V-Act/assets/3820588/fbcde8d1-a7d6-4089-95f6-fd099cc98a0d\n## How it works\nGPT-4V-Act leverages both [GPT-4V(ision)](https://openai.com/research/gpt-4v-system-card) and [Set-of-Mark Prompting](https://arxiv.org/abs/2310.11441), together with a tailored auto-labeler. This auto-labeler assigns a unique numerical ID to each interactable UI element.\nBy incorporating a task and a screenshot as input, GPT-4V-Act can deduce the subsequent action required to accomplish a task. For mouse/keyboard output, it can refer to the numerical labels for exact pixel coordinates.\n**Get Started!**\n```bash\n# Clone the repo\ngit clone https://github.com/ddupont808/GPT-4V-Act ai-browser\n# Navigate to the repo directory\ncd ai-browser\n# Install the required packages\nnpm install\n# Start the demo\nnpm start\n```\n## Features\n- \ud83d\udd04 Vision (Partial) \n   - \u2705 JS DOM auto-labeler (w/ COCO export)  \n   - \u274c AI auto-labeler\n- \u2705 Clicking \n- \ud83d\udd04 Typing (Partial) \n   - \u2705 Typing characters (letters, numbers, strings) \n   - \u274c Typing special keycodes (enter, pgup, pgdown)"
        },
        {
            "comment": "This code defines action types and response format for instructing the agent to perform tasks on a website. It provides an example demonstration prompt and instructions for observation and next actions.",
            "location": "\"/media/root/Toshiba XG3/works/GPT-4V-Act/docs/src/README.md\":39-72",
            "content": "- \u274c Scrolling\n- \u274c Prompting user for more information\n- \u274c Remembering information relevant to task\nIf you have ideas, feedback, or want to contribute, feel free to create an Issue or reach out to ddupont@mit.edu\n## Demonstration Prompt\nBelow is an example of using the user-interface to instruct the agent to \"play a random song for me\"\n![](demo1.png)\nThis is the prompt seen by GPT-4V and the corresponding output\n### User\n![](demo2.png)\n```markdown\ntask: play a random song for me\ntype ClickAction = { action: \"click\", element: number }\ntype TypeAction = { action: \"type\", element: number, text: string }\ntype ScrollAction = { action: \"scroll\", direction: \"up\" | \"down\" }\ntype RequestInfoFromUser = { action: \"request-info\", prompt: string }\ntype RememberInfoFromSite = { action: \"remember-info\", info: string }\ntype Done = { action: \"done\" }\n## response format\n{\n  briefExplanation: string,\n  nextAction: ClickAction | TypeAction | ScrollAction | RequestInfoFromUser | RememberInfoFromSite | Done\n}\n## instructions\n# observe the screenshot, and think about the next action"
        },
        {
            "comment": "This code is asking the AI to output its response in a JSON markdown code block. The JSON object contains a brief explanation of the action (\"I'll type 'random song' into the search bar to find a song for you.\") and a nextAction object with an action of \"type\", an element of 7, and text of \"random song\".",
            "location": "\"/media/root/Toshiba XG3/works/GPT-4V-Act/docs/src/README.md\":73-82",
            "content": "# output your response in a json markdown code block\n```\n### Assistant\n```json\n{\n  \"briefExplanation\": \"I'll type 'random song' into the search bar to find a song for you.\",\n  \"nextAction\": { \"action\": \"type\", \"element\": 7, \"text\": \"random song\" }\n}\n```"
        }
    ]
}